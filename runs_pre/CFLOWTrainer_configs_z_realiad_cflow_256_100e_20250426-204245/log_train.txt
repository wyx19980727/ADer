2025-04-26 20:42:45,188 - ==> Logging on master GPU: 0
2025-04-26 20:42:45,188 - ==> Running Trainer: CFLOWTrainer
2025-04-26 20:42:45,188 - ==> Using GPU: [0] for Training
2025-04-26 20:42:45,188 - ==> Building model
2025-04-26 20:42:46,270 - Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/wide_resnet50_racm-8234f177.pth)
2025-04-26 20:43:57,130 - 
------------------------------------ CFLOW ------------------------------------
| module                                     | #parameters or shape    | #flops   |
|:-------------------------------------------|:------------------------|:---------|
| model                                      | 0.237G                  | 0        |
|  encoder                                   |  66.834M                |          |
|   encoder.conv1                            |   9.408K                |          |
|    encoder.conv1.weight                    |    (64, 3, 7, 7)        |          |
|   encoder.bn1                              |   0.128K                |          |
|    encoder.bn1.weight                      |    (64,)                |          |
|    encoder.bn1.bias                        |    (64,)                |          |
|   encoder.layer1                           |   0.634M                |          |
|    encoder.layer1.0                        |    0.206M               |          |
|     encoder.layer1.0.conv1                 |     8.192K              |          |
|      encoder.layer1.0.conv1.weight         |      (128, 64, 1, 1)    |          |
|     encoder.layer1.0.bn1                   |     0.256K              |          |
|      encoder.layer1.0.bn1.weight           |      (128,)             |          |
|      encoder.layer1.0.bn1.bias             |      (128,)             |          |
|     encoder.layer1.0.conv2                 |     0.147M              |          |
|      encoder.layer1.0.conv2.weight         |      (128, 128, 3, 3)   |          |
|     encoder.layer1.0.bn2                   |     0.256K              |          |
|      encoder.layer1.0.bn2.weight           |      (128,)             |          |
|      encoder.layer1.0.bn2.bias             |      (128,)             |          |
|     encoder.layer1.0.conv3                 |     32.768K             |          |
|      encoder.layer1.0.conv3.weight         |      (256, 128, 1, 1)   |          |
|     encoder.layer1.0.bn3                   |     0.512K              |          |
|      encoder.layer1.0.bn3.weight           |      (256,)             |          |
|      encoder.layer1.0.bn3.bias             |      (256,)             |          |
|     encoder.layer1.0.downsample            |     16.896K             |          |
|      encoder.layer1.0.downsample.0         |      16.384K            |          |
|      encoder.layer1.0.downsample.1         |      0.512K             |          |
|    encoder.layer1.1                        |    0.214M               |          |
|     encoder.layer1.1.conv1                 |     32.768K             |          |
|      encoder.layer1.1.conv1.weight         |      (128, 256, 1, 1)   |          |
|     encoder.layer1.1.bn1                   |     0.256K              |          |
|      encoder.layer1.1.bn1.weight           |      (128,)             |          |
|      encoder.layer1.1.bn1.bias             |      (128,)             |          |
|     encoder.layer1.1.conv2                 |     0.147M              |          |
|      encoder.layer1.1.conv2.weight         |      (128, 128, 3, 3)   |          |
|     encoder.layer1.1.bn2                   |     0.256K              |          |
|      encoder.layer1.1.bn2.weight           |      (128,)             |          |
|      encoder.layer1.1.bn2.bias             |      (128,)             |          |
|     encoder.layer1.1.conv3                 |     32.768K             |          |
|      encoder.layer1.1.conv3.weight         |      (256, 128, 1, 1)   |          |
|     encoder.layer1.1.bn3                   |     0.512K              |          |
|      encoder.layer1.1.bn3.weight           |      (256,)             |          |
|      encoder.layer1.1.bn3.bias             |      (256,)             |          |
|    encoder.layer1.2                        |    0.214M               |          |
|     encoder.layer1.2.conv1                 |     32.768K             |          |
|      encoder.layer1.2.conv1.weight         |      (128, 256, 1, 1)   |          |
|     encoder.layer1.2.bn1                   |     0.256K              |          |
|      encoder.layer1.2.bn1.weight           |      (128,)             |          |
|      encoder.layer1.2.bn1.bias             |      (128,)             |          |
|     encoder.layer1.2.conv2                 |     0.147M              |          |
|      encoder.layer1.2.conv2.weight         |      (128, 128, 3, 3)   |          |
|     encoder.layer1.2.bn2                   |     0.256K              |          |
|      encoder.layer1.2.bn2.weight           |      (128,)             |          |
|      encoder.layer1.2.bn2.bias             |      (128,)             |          |
|     encoder.layer1.2.conv3                 |     32.768K             |          |
|      encoder.layer1.2.conv3.weight         |      (256, 128, 1, 1)   |          |
|     encoder.layer1.2.bn3                   |     0.512K              |          |
|      encoder.layer1.2.bn3.weight           |      (256,)             |          |
|      encoder.layer1.2.bn3.bias             |      (256,)             |          |
|   encoder.layer2                           |   3.483M                |          |
|    encoder.layer2.0                        |    0.921M               |          |
|     encoder.layer2.0.conv1                 |     65.536K             |          |
|      encoder.layer2.0.conv1.weight         |      (256, 256, 1, 1)   |          |
|     encoder.layer2.0.bn1                   |     0.512K              |          |
|      encoder.layer2.0.bn1.weight           |      (256,)             |          |
|      encoder.layer2.0.bn1.bias             |      (256,)             |          |
|     encoder.layer2.0.conv2                 |     0.59M               |          |
|      encoder.layer2.0.conv2.weight         |      (256, 256, 3, 3)   |          |
|     encoder.layer2.0.bn2                   |     0.512K              |          |
|      encoder.layer2.0.bn2.weight           |      (256,)             |          |
|      encoder.layer2.0.bn2.bias             |      (256,)             |          |
|     encoder.layer2.0.conv3                 |     0.131M              |          |
|      encoder.layer2.0.conv3.weight         |      (512, 256, 1, 1)   |          |
|     encoder.layer2.0.bn3                   |     1.024K              |          |
|      encoder.layer2.0.bn3.weight           |      (512,)             |          |
|      encoder.layer2.0.bn3.bias             |      (512,)             |          |
|     encoder.layer2.0.downsample            |     0.132M              |          |
|      encoder.layer2.0.downsample.0         |      0.131M             |          |
|      encoder.layer2.0.downsample.1         |      1.024K             |          |
|    encoder.layer2.1                        |    0.854M               |          |
|     encoder.layer2.1.conv1                 |     0.131M              |          |
|      encoder.layer2.1.conv1.weight         |      (256, 512, 1, 1)   |          |
|     encoder.layer2.1.bn1                   |     0.512K              |          |
|      encoder.layer2.1.bn1.weight           |      (256,)             |          |
|      encoder.layer2.1.bn1.bias             |      (256,)             |          |
|     encoder.layer2.1.conv2                 |     0.59M               |          |
|      encoder.layer2.1.conv2.weight         |      (256, 256, 3, 3)   |          |
|     encoder.layer2.1.bn2                   |     0.512K              |          |
|      encoder.layer2.1.bn2.weight           |      (256,)             |          |
|      encoder.layer2.1.bn2.bias             |      (256,)             |          |
|     encoder.layer2.1.conv3                 |     0.131M              |          |
|      encoder.layer2.1.conv3.weight         |      (512, 256, 1, 1)   |          |
|     encoder.layer2.1.bn3                   |     1.024K              |          |
|      encoder.layer2.1.bn3.weight           |      (512,)             |          |
|      encoder.layer2.1.bn3.bias             |      (512,)             |          |
|    encoder.layer2.2                        |    0.854M               |          |
|     encoder.layer2.2.conv1                 |     0.131M              |          |
|      encoder.layer2.2.conv1.weight         |      (256, 512, 1, 1)   |          |
|     encoder.layer2.2.bn1                   |     0.512K              |          |
|      encoder.layer2.2.bn1.weight           |      (256,)             |          |
|      encoder.layer2.2.bn1.bias             |      (256,)             |          |
|     encoder.layer2.2.conv2                 |     0.59M               |          |
|      encoder.layer2.2.conv2.weight         |      (256, 256, 3, 3)   |          |
|     encoder.layer2.2.bn2                   |     0.512K              |          |
|      encoder.layer2.2.bn2.weight           |      (256,)             |          |
|      encoder.layer2.2.bn2.bias             |      (256,)             |          |
|     encoder.layer2.2.conv3                 |     0.131M              |          |
|      encoder.layer2.2.conv3.weight         |      (512, 256, 1, 1)   |          |
|     encoder.layer2.2.bn3                   |     1.024K              |          |
|      encoder.layer2.2.bn3.weight           |      (512,)             |          |
|      encoder.layer2.2.bn3.bias             |      (512,)             |          |
|    encoder.layer2.3                        |    0.854M               |          |
|     encoder.layer2.3.conv1                 |     0.131M              |          |
|      encoder.layer2.3.conv1.weight         |      (256, 512, 1, 1)   |          |
|     encoder.layer2.3.bn1                   |     0.512K              |          |
|      encoder.layer2.3.bn1.weight           |      (256,)             |          |
|      encoder.layer2.3.bn1.bias             |      (256,)             |          |
|     encoder.layer2.3.conv2                 |     0.59M               |          |
|      encoder.layer2.3.conv2.weight         |      (256, 256, 3, 3)   |          |
|     encoder.layer2.3.bn2                   |     0.512K              |          |
|      encoder.layer2.3.bn2.weight           |      (256,)             |          |
|      encoder.layer2.3.bn2.bias             |      (256,)             |          |
|     encoder.layer2.3.conv3                 |     0.131M              |          |
|      encoder.layer2.3.conv3.weight         |      (512, 256, 1, 1)   |          |
|     encoder.layer2.3.bn3                   |     1.024K              |          |
|      encoder.layer2.3.bn3.weight           |      (512,)             |          |
|      encoder.layer2.3.bn3.bias             |      (512,)             |          |
|   encoder.layer3                           |   20.736M               |          |
|    encoder.layer3.0                        |    3.676M               |          |
|     encoder.layer3.0.conv1                 |     0.262M              |          |
|      encoder.layer3.0.conv1.weight         |      (512, 512, 1, 1)   |          |
|     encoder.layer3.0.bn1                   |     1.024K              |          |
|      encoder.layer3.0.bn1.weight           |      (512,)             |          |
|      encoder.layer3.0.bn1.bias             |      (512,)             |          |
|     encoder.layer3.0.conv2                 |     2.359M              |          |
|      encoder.layer3.0.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.0.bn2                   |     1.024K              |          |
|      encoder.layer3.0.bn2.weight           |      (512,)             |          |
|      encoder.layer3.0.bn2.bias             |      (512,)             |          |
|     encoder.layer3.0.conv3                 |     0.524M              |          |
|      encoder.layer3.0.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.0.bn3                   |     2.048K              |          |
|      encoder.layer3.0.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.0.bn3.bias             |      (1024,)            |          |
|     encoder.layer3.0.downsample            |     0.526M              |          |
|      encoder.layer3.0.downsample.0         |      0.524M             |          |
|      encoder.layer3.0.downsample.1         |      2.048K             |          |
|    encoder.layer3.1                        |    3.412M               |          |
|     encoder.layer3.1.conv1                 |     0.524M              |          |
|      encoder.layer3.1.conv1.weight         |      (512, 1024, 1, 1)  |          |
|     encoder.layer3.1.bn1                   |     1.024K              |          |
|      encoder.layer3.1.bn1.weight           |      (512,)             |          |
|      encoder.layer3.1.bn1.bias             |      (512,)             |          |
|     encoder.layer3.1.conv2                 |     2.359M              |          |
|      encoder.layer3.1.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.1.bn2                   |     1.024K              |          |
|      encoder.layer3.1.bn2.weight           |      (512,)             |          |
|      encoder.layer3.1.bn2.bias             |      (512,)             |          |
|     encoder.layer3.1.conv3                 |     0.524M              |          |
|      encoder.layer3.1.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.1.bn3                   |     2.048K              |          |
|      encoder.layer3.1.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.1.bn3.bias             |      (1024,)            |          |
|    encoder.layer3.2                        |    3.412M               |          |
|     encoder.layer3.2.conv1                 |     0.524M              |          |
|      encoder.layer3.2.conv1.weight         |      (512, 1024, 1, 1)  |          |
|     encoder.layer3.2.bn1                   |     1.024K              |          |
|      encoder.layer3.2.bn1.weight           |      (512,)             |          |
|      encoder.layer3.2.bn1.bias             |      (512,)             |          |
|     encoder.layer3.2.conv2                 |     2.359M              |          |
|      encoder.layer3.2.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.2.bn2                   |     1.024K              |          |
|      encoder.layer3.2.bn2.weight           |      (512,)             |          |
|      encoder.layer3.2.bn2.bias             |      (512,)             |          |
|     encoder.layer3.2.conv3                 |     0.524M              |          |
|      encoder.layer3.2.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.2.bn3                   |     2.048K              |          |
|      encoder.layer3.2.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.2.bn3.bias             |      (1024,)            |          |
|    encoder.layer3.3                        |    3.412M               |          |
|     encoder.layer3.3.conv1                 |     0.524M              |          |
|      encoder.layer3.3.conv1.weight         |      (512, 1024, 1, 1)  |          |
|     encoder.layer3.3.bn1                   |     1.024K              |          |
|      encoder.layer3.3.bn1.weight           |      (512,)             |          |
|      encoder.layer3.3.bn1.bias             |      (512,)             |          |
|     encoder.layer3.3.conv2                 |     2.359M              |          |
|      encoder.layer3.3.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.3.bn2                   |     1.024K              |          |
|      encoder.layer3.3.bn2.weight           |      (512,)             |          |
|      encoder.layer3.3.bn2.bias             |      (512,)             |          |
|     encoder.layer3.3.conv3                 |     0.524M              |          |
|      encoder.layer3.3.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.3.bn3                   |     2.048K              |          |
|      encoder.layer3.3.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.3.bn3.bias             |      (1024,)            |          |
|    encoder.layer3.4                        |    3.412M               |          |
|     encoder.layer3.4.conv1                 |     0.524M              |          |
|      encoder.layer3.4.conv1.weight         |      (512, 1024, 1, 1)  |          |
|     encoder.layer3.4.bn1                   |     1.024K              |          |
|      encoder.layer3.4.bn1.weight           |      (512,)             |          |
|      encoder.layer3.4.bn1.bias             |      (512,)             |          |
|     encoder.layer3.4.conv2                 |     2.359M              |          |
|      encoder.layer3.4.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.4.bn2                   |     1.024K              |          |
|      encoder.layer3.4.bn2.weight           |      (512,)             |          |
|      encoder.layer3.4.bn2.bias             |      (512,)             |          |
|     encoder.layer3.4.conv3                 |     0.524M              |          |
|      encoder.layer3.4.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.4.bn3                   |     2.048K              |          |
|      encoder.layer3.4.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.4.bn3.bias             |      (1024,)            |          |
|    encoder.layer3.5                        |    3.412M               |          |
|     encoder.layer3.5.conv1                 |     0.524M              |          |
|      encoder.layer3.5.conv1.weight         |      (512, 1024, 1, 1)  |          |
|     encoder.layer3.5.bn1                   |     1.024K              |          |
|      encoder.layer3.5.bn1.weight           |      (512,)             |          |
|      encoder.layer3.5.bn1.bias             |      (512,)             |          |
|     encoder.layer3.5.conv2                 |     2.359M              |          |
|      encoder.layer3.5.conv2.weight         |      (512, 512, 3, 3)   |          |
|     encoder.layer3.5.bn2                   |     1.024K              |          |
|      encoder.layer3.5.bn2.weight           |      (512,)             |          |
|      encoder.layer3.5.bn2.bias             |      (512,)             |          |
|     encoder.layer3.5.conv3                 |     0.524M              |          |
|      encoder.layer3.5.conv3.weight         |      (1024, 512, 1, 1)  |          |
|     encoder.layer3.5.bn3                   |     2.048K              |          |
|      encoder.layer3.5.bn3.weight           |      (1024,)            |          |
|      encoder.layer3.5.bn3.bias             |      (1024,)            |          |
|   encoder.layer4                           |   41.972M               |          |
|    encoder.layer4.0                        |    14.692M              |          |
|     encoder.layer4.0.conv1                 |     1.049M              |          |
|      encoder.layer4.0.conv1.weight         |      (1024, 1024, 1, 1) |          |
|     encoder.layer4.0.bn1                   |     2.048K              |          |
|      encoder.layer4.0.bn1.weight           |      (1024,)            |          |
|      encoder.layer4.0.bn1.bias             |      (1024,)            |          |
|     encoder.layer4.0.conv2                 |     9.437M              |          |
|      encoder.layer4.0.conv2.weight         |      (1024, 1024, 3, 3) |          |
|     encoder.layer4.0.bn2                   |     2.048K              |          |
|      encoder.layer4.0.bn2.weight           |      (1024,)            |          |
|      encoder.layer4.0.bn2.bias             |      (1024,)            |          |
|     encoder.layer4.0.conv3                 |     2.097M              |          |
|      encoder.layer4.0.conv3.weight         |      (2048, 1024, 1, 1) |          |
|     encoder.layer4.0.bn3                   |     4.096K              |          |
|      encoder.layer4.0.bn3.weight           |      (2048,)            |          |
|      encoder.layer4.0.bn3.bias             |      (2048,)            |          |
|     encoder.layer4.0.downsample            |     2.101M              |          |
|      encoder.layer4.0.downsample.0         |      2.097M             |          |
|      encoder.layer4.0.downsample.1         |      4.096K             |          |
|    encoder.layer4.1                        |    13.64M               |          |
|     encoder.layer4.1.conv1                 |     2.097M              |          |
|      encoder.layer4.1.conv1.weight         |      (1024, 2048, 1, 1) |          |
|     encoder.layer4.1.bn1                   |     2.048K              |          |
|      encoder.layer4.1.bn1.weight           |      (1024,)            |          |
|      encoder.layer4.1.bn1.bias             |      (1024,)            |          |
|     encoder.layer4.1.conv2                 |     9.437M              |          |
|      encoder.layer4.1.conv2.weight         |      (1024, 1024, 3, 3) |          |
|     encoder.layer4.1.bn2                   |     2.048K              |          |
|      encoder.layer4.1.bn2.weight           |      (1024,)            |          |
|      encoder.layer4.1.bn2.bias             |      (1024,)            |          |
|     encoder.layer4.1.conv3                 |     2.097M              |          |
|      encoder.layer4.1.conv3.weight         |      (2048, 1024, 1, 1) |          |
|     encoder.layer4.1.bn3                   |     4.096K              |          |
|      encoder.layer4.1.bn3.weight           |      (2048,)            |          |
|      encoder.layer4.1.bn3.bias             |      (2048,)            |          |
|    encoder.layer4.2                        |    13.64M               |          |
|     encoder.layer4.2.conv1                 |     2.097M              |          |
|      encoder.layer4.2.conv1.weight         |      (1024, 2048, 1, 1) |          |
|     encoder.layer4.2.bn1                   |     2.048K              |          |
|      encoder.layer4.2.bn1.weight           |      (1024,)            |          |
|      encoder.layer4.2.bn1.bias             |      (1024,)            |          |
|     encoder.layer4.2.conv2                 |     9.437M              |          |
|      encoder.layer4.2.conv2.weight         |      (1024, 1024, 3, 3) |          |
|     encoder.layer4.2.bn2                   |     2.048K              |          |
|      encoder.layer4.2.bn2.weight           |      (1024,)            |          |
|      encoder.layer4.2.bn2.bias             |      (1024,)            |          |
|     encoder.layer4.2.conv3                 |     2.097M              |          |
|      encoder.layer4.2.conv3.weight         |      (2048, 1024, 1, 1) |          |
|     encoder.layer4.2.bn3                   |     4.096K              |          |
|      encoder.layer4.2.bn3.weight           |      (2048,)            |          |
|      encoder.layer4.2.bn3.bias             |      (2048,)            |          |
|  decoders                                  |  0.17G                  |          |
|   decoders.0.module_list                   |   9.718M                |          |
|    decoders.0.module_list.0                |    1.215M               |          |
|     decoders.0.module_list.0.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.0.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.0.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.0.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.0.subnet        |     0.689M              |          |
|      decoders.0.module_list.0.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.0.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.1                |    1.215M               |          |
|     decoders.0.module_list.1.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.1.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.1.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.1.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.1.subnet        |     0.689M              |          |
|      decoders.0.module_list.1.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.1.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.2                |    1.215M               |          |
|     decoders.0.module_list.2.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.2.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.2.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.2.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.2.subnet        |     0.689M              |          |
|      decoders.0.module_list.2.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.2.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.3                |    1.215M               |          |
|     decoders.0.module_list.3.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.3.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.3.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.3.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.3.subnet        |     0.689M              |          |
|      decoders.0.module_list.3.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.3.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.4                |    1.215M               |          |
|     decoders.0.module_list.4.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.4.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.4.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.4.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.4.subnet        |     0.689M              |          |
|      decoders.0.module_list.4.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.4.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.5                |    1.215M               |          |
|     decoders.0.module_list.5.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.5.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.5.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.5.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.5.subnet        |     0.689M              |          |
|      decoders.0.module_list.5.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.5.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.6                |    1.215M               |          |
|     decoders.0.module_list.6.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.6.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.6.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.6.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.6.subnet        |     0.689M              |          |
|      decoders.0.module_list.6.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.6.subnet.2     |      0.394M             |          |
|    decoders.0.module_list.7                |    1.215M               |          |
|     decoders.0.module_list.7.global_scale  |     (1, 512)            |          |
|     decoders.0.module_list.7.global_offset |     (1, 512)            |          |
|     decoders.0.module_list.7.w_perm        |     (512, 512)          |          |
|     decoders.0.module_list.7.w_perm_inv    |     (512, 512)          |          |
|     decoders.0.module_list.7.subnet        |     0.689M              |          |
|      decoders.0.module_list.7.subnet.0     |      0.296M             |          |
|      decoders.0.module_list.7.subnet.2     |      0.394M             |          |
|   decoders.1.module_list                   |   33.851M               |          |
|    decoders.1.module_list.0                |    4.231M               |          |
|     decoders.1.module_list.0.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.0.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.0.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.0.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.0.subnet        |     2.132M              |          |
|      decoders.1.module_list.0.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.0.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.1                |    4.231M               |          |
|     decoders.1.module_list.1.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.1.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.1.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.1.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.1.subnet        |     2.132M              |          |
|      decoders.1.module_list.1.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.1.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.2                |    4.231M               |          |
|     decoders.1.module_list.2.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.2.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.2.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.2.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.2.subnet        |     2.132M              |          |
|      decoders.1.module_list.2.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.2.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.3                |    4.231M               |          |
|     decoders.1.module_list.3.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.3.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.3.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.3.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.3.subnet        |     2.132M              |          |
|      decoders.1.module_list.3.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.3.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.4                |    4.231M               |          |
|     decoders.1.module_list.4.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.4.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.4.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.4.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.4.subnet        |     2.132M              |          |
|      decoders.1.module_list.4.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.4.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.5                |    4.231M               |          |
|     decoders.1.module_list.5.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.5.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.5.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.5.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.5.subnet        |     2.132M              |          |
|      decoders.1.module_list.5.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.5.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.6                |    4.231M               |          |
|     decoders.1.module_list.6.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.6.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.6.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.6.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.6.subnet        |     2.132M              |          |
|      decoders.1.module_list.6.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.6.subnet.2     |      1.312M             |          |
|    decoders.1.module_list.7                |    4.231M               |          |
|     decoders.1.module_list.7.global_scale  |     (1, 1024)           |          |
|     decoders.1.module_list.7.global_offset |     (1, 1024)           |          |
|     decoders.1.module_list.7.w_perm        |     (1024, 1024)        |          |
|     decoders.1.module_list.7.w_perm_inv    |     (1024, 1024)        |          |
|     decoders.1.module_list.7.subnet        |     2.132M              |          |
|      decoders.1.module_list.7.subnet.0     |      0.82M              |          |
|      decoders.1.module_list.7.subnet.2     |      1.312M             |          |
|   decoders.2.module_list                   |   0.126G                |          |
|    decoders.2.module_list.0                |    15.77M               |          |
|     decoders.2.module_list.0.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.0.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.0.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.0.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.0.subnet        |     7.377M              |          |
|      decoders.2.module_list.0.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.0.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.1                |    15.77M               |          |
|     decoders.2.module_list.1.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.1.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.1.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.1.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.1.subnet        |     7.377M              |          |
|      decoders.2.module_list.1.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.1.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.2                |    15.77M               |          |
|     decoders.2.module_list.2.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.2.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.2.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.2.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.2.subnet        |     7.377M              |          |
|      decoders.2.module_list.2.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.2.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.3                |    15.77M               |          |
|     decoders.2.module_list.3.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.3.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.3.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.3.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.3.subnet        |     7.377M              |          |
|      decoders.2.module_list.3.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.3.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.4                |    15.77M               |          |
|     decoders.2.module_list.4.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.4.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.4.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.4.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.4.subnet        |     7.377M              |          |
|      decoders.2.module_list.4.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.4.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.5                |    15.77M               |          |
|     decoders.2.module_list.5.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.5.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.5.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.5.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.5.subnet        |     7.377M              |          |
|      decoders.2.module_list.5.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.5.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.6                |    15.77M               |          |
|     decoders.2.module_list.6.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.6.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.6.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.6.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.6.subnet        |     7.377M              |          |
|      decoders.2.module_list.6.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.6.subnet.2     |      4.721M             |          |
|    decoders.2.module_list.7                |    15.77M               |          |
|     decoders.2.module_list.7.global_scale  |     (1, 2048)           |          |
|     decoders.2.module_list.7.global_offset |     (1, 2048)           |          |
|     decoders.2.module_list.7.w_perm        |     (2048, 2048)        |          |
|     decoders.2.module_list.7.w_perm_inv    |     (2048, 2048)        |          |
|     decoders.2.module_list.7.subnet        |     7.377M              |          |
|      decoders.2.module_list.7.subnet.0     |      2.657M             |          |
|      decoders.2.module_list.7.subnet.2     |      4.721M             |          |
-------------------------------------------------------------------------------
2025-04-26 20:43:57,130 - ==> Creating optimizer
2025-04-26 20:43:57,131 - ==> Loading dataset: RealIAD
2025-04-26 20:43:57,417 - ==> ********** cfg ********** 
fvcore_is                            : True                                
fvcore_b                             : 1                                   
fvcore_c                             : 3                                   
epoch_full                           : 10                                  
metrics                              : ['S_AUROC', 'S_AUPR', 'mAUROC_sp_max', 'mAP_sp_max', 'mF1_max_sp_max', 'mAUROC_px', 'mAP_px', 'mF1_max_px', 'mAUPRO_px', 'mIoU_max_px']
use_adeval                           : True                                
evaluator.kwargs                     : {'metrics': ['S_AUROC', 'S_AUPR', 'mAUROC_sp_max', 'mAP_sp_max', 'mF1_max_sp_max', 'mAUROC_px', 'mAP_px', 'mF1_max_px', 'mAUPRO_px', 'mIoU_max_px'], 'pooling_ks': [16, 16], 'max_step_aupro': 100, 'use_adeval': True}
optim.lr                             : 0.0001                              
optim.kwargs                         : {'name': 'adamw', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False}
trainer.name                         : CFLOWTrainer                        
trainer.checkpoint                   : runs                                
trainer.logdir_sub                   :                                     
trainer.resume_dir                   :                                     
trainer.cuda_deterministic           : False                               
trainer.epoch_full                   : 10                                  
trainer.scheduler_kwargs             : {'name': 'step', 'lr_noise': None, 'noise_pct': 0.67, 'noise_std': 1.0, 'noise_seed': 42, 'lr_min': 1e-06, 'warmup_lr': 1.0000000000000001e-07, 'warmup_iters': -1, 'cooldown_iters': 0, 'warmup_epochs': 0, 'cooldown_epochs': 0, 'use_iters': True, 'patience_iters': 0, 'patience_epochs': 0, 'decay_iters': 0, 'decay_epochs': 8, 'cycle_decay': 0.1, 'decay_rate': 0.1}
trainer.mixup_kwargs                 : {'mixup_alpha': 0.8, 'cutmix_alpha': 1.0, 'cutmix_minmax': None, 'prob': 0.0, 'switch_prob': 0.5, 'mode': 'batch', 'correct_lam': True, 'label_smoothing': 0.1}
trainer.test_start_epoch             : 100                                 
trainer.test_per_epoch               : 10                                  
trainer.find_unused_parameters       : False                               
trainer.sync_BN                      : apex                                
trainer.dist_BN                      :                                     
trainer.scaler                       : none                                
trainer.data.batch_size              : 8                                   
trainer.data.batch_size_per_gpu      : 8                                   
trainer.data.batch_size_test         : 8                                   
trainer.data.batch_size_per_gpu_test : 8                                   
trainer.data.num_workers_per_gpu     : 8                                   
trainer.data.drop_last               : True                                
trainer.data.pin_memory              : True                                
trainer.data.persistent_workers      : False                               
trainer.data.num_workers             : 8                                   
trainer.lr                           : 0.0001                              
trainer.lr_warm_epochs               : 2                                   
trainer.lr_cosine                    : True                                
trainer.lr_warm                      : True                                
trainer.lr_decay_rate                : 0.1                                 
trainer.meta_epochs                  : 10                                  
trainer.sub_epochs                   : 10                                  
trainer.lr_decay_epochs              : [5, 7, 9]                           
trainer.lr_warmup_from               : 1e-05                               
trainer.lr_warmup_to                 : 9.046039886902862e-05               
trainer.iter                         : 0                                   
trainer.epoch                        : 0                                   
trainer.iter_full                    : 9110                                
trainer.metric_recorder              : {'S_AUROC_audiojack': [], 'S_AUPR_audiojack': [], 'mAUROC_sp_max_audiojack': [], 'mAP_sp_max_audiojack': [], 'mF1_max_sp_max_audiojack': [], 'mAUROC_px_audiojack': [], 'mAP_px_audiojack': [], 'mF1_max_px_audiojack': [], 'mAUPRO_px_audiojack': [], 'mIoU_max_px_audiojack': [], 'S_AUROC_bottle_cap': [], 'S_AUPR_bottle_cap': [], 'mAUROC_sp_max_bottle_cap': [], 'mAP_sp_max_bottle_cap': [], 'mF1_max_sp_max_bottle_cap': [], 'mAUROC_px_bottle_cap': [], 'mAP_px_bottle_cap': [], 'mF1_max_px_bottle_cap': [], 'mAUPRO_px_bottle_cap': [], 'mIoU_max_px_bottle_cap': [], 'S_AUROC_button_battery': [], 'S_AUPR_button_battery': [], 'mAUROC_sp_max_button_battery': [], 'mAP_sp_max_button_battery': [], 'mF1_max_sp_max_button_battery': [], 'mAUROC_px_button_battery': [], 'mAP_px_button_battery': [], 'mF1_max_px_button_battery': [], 'mAUPRO_px_button_battery': [], 'mIoU_max_px_button_battery': [], 'S_AUROC_end_cap': [], 'S_AUPR_end_cap': [], 'mAUROC_sp_max_end_cap': [], 'mAP_sp_max_end_cap': [], 'mF1_max_sp_max_end_cap': [], 'mAUROC_px_end_cap': [], 'mAP_px_end_cap': [], 'mF1_max_px_end_cap': [], 'mAUPRO_px_end_cap': [], 'mIoU_max_px_end_cap': [], 'S_AUROC_eraser': [], 'S_AUPR_eraser': [], 'mAUROC_sp_max_eraser': [], 'mAP_sp_max_eraser': [], 'mF1_max_sp_max_eraser': [], 'mAUROC_px_eraser': [], 'mAP_px_eraser': [], 'mF1_max_px_eraser': [], 'mAUPRO_px_eraser': [], 'mIoU_max_px_eraser': [], 'S_AUROC_fire_hood': [], 'S_AUPR_fire_hood': [], 'mAUROC_sp_max_fire_hood': [], 'mAP_sp_max_fire_hood': [], 'mF1_max_sp_max_fire_hood': [], 'mAUROC_px_fire_hood': [], 'mAP_px_fire_hood': [], 'mF1_max_px_fire_hood': [], 'mAUPRO_px_fire_hood': [], 'mIoU_max_px_fire_hood': [], 'S_AUROC_mint': [], 'S_AUPR_mint': [], 'mAUROC_sp_max_mint': [], 'mAP_sp_max_mint': [], 'mF1_max_sp_max_mint': [], 'mAUROC_px_mint': [], 'mAP_px_mint': [], 'mF1_max_px_mint': [], 'mAUPRO_px_mint': [], 'mIoU_max_px_mint': [], 'S_AUROC_mounts': [], 'S_AUPR_mounts': [], 'mAUROC_sp_max_mounts': [], 'mAP_sp_max_mounts': [], 'mF1_max_sp_max_mounts': [], 'mAUROC_px_mounts': [], 'mAP_px_mounts': [], 'mF1_max_px_mounts': [], 'mAUPRO_px_mounts': [], 'mIoU_max_px_mounts': [], 'S_AUROC_pcb': [], 'S_AUPR_pcb': [], 'mAUROC_sp_max_pcb': [], 'mAP_sp_max_pcb': [], 'mF1_max_sp_max_pcb': [], 'mAUROC_px_pcb': [], 'mAP_px_pcb': [], 'mF1_max_px_pcb': [], 'mAUPRO_px_pcb': [], 'mIoU_max_px_pcb': [], 'S_AUROC_phone_battery': [], 'S_AUPR_phone_battery': [], 'mAUROC_sp_max_phone_battery': [], 'mAP_sp_max_phone_battery': [], 'mF1_max_sp_max_phone_battery': [], 'mAUROC_px_phone_battery': [], 'mAP_px_phone_battery': [], 'mF1_max_px_phone_battery': [], 'mAUPRO_px_phone_battery': [], 'mIoU_max_px_phone_battery': [], 'S_AUROC_plastic_nut': [], 'S_AUPR_plastic_nut': [], 'mAUROC_sp_max_plastic_nut': [], 'mAP_sp_max_plastic_nut': [], 'mF1_max_sp_max_plastic_nut': [], 'mAUROC_px_plastic_nut': [], 'mAP_px_plastic_nut': [], 'mF1_max_px_plastic_nut': [], 'mAUPRO_px_plastic_nut': [], 'mIoU_max_px_plastic_nut': [], 'S_AUROC_plastic_plug': [], 'S_AUPR_plastic_plug': [], 'mAUROC_sp_max_plastic_plug': [], 'mAP_sp_max_plastic_plug': [], 'mF1_max_sp_max_plastic_plug': [], 'mAUROC_px_plastic_plug': [], 'mAP_px_plastic_plug': [], 'mF1_max_px_plastic_plug': [], 'mAUPRO_px_plastic_plug': [], 'mIoU_max_px_plastic_plug': [], 'S_AUROC_porcelain_doll': [], 'S_AUPR_porcelain_doll': [], 'mAUROC_sp_max_porcelain_doll': [], 'mAP_sp_max_porcelain_doll': [], 'mF1_max_sp_max_porcelain_doll': [], 'mAUROC_px_porcelain_doll': [], 'mAP_px_porcelain_doll': [], 'mF1_max_px_porcelain_doll': [], 'mAUPRO_px_porcelain_doll': [], 'mIoU_max_px_porcelain_doll': [], 'S_AUROC_regulator': [], 'S_AUPR_regulator': [], 'mAUROC_sp_max_regulator': [], 'mAP_sp_max_regulator': [], 'mF1_max_sp_max_regulator': [], 'mAUROC_px_regulator': [], 'mAP_px_regulator': [], 'mF1_max_px_regulator': [], 'mAUPRO_px_regulator': [], 'mIoU_max_px_regulator': [], 'S_AUROC_rolled_strip_base': [], 'S_AUPR_rolled_strip_base': [], 'mAUROC_sp_max_rolled_strip_base': [], 'mAP_sp_max_rolled_strip_base': [], 'mF1_max_sp_max_rolled_strip_base': [], 'mAUROC_px_rolled_strip_base': [], 'mAP_px_rolled_strip_base': [], 'mF1_max_px_rolled_strip_base': [], 'mAUPRO_px_rolled_strip_base': [], 'mIoU_max_px_rolled_strip_base': [], 'S_AUROC_sim_card_set': [], 'S_AUPR_sim_card_set': [], 'mAUROC_sp_max_sim_card_set': [], 'mAP_sp_max_sim_card_set': [], 'mF1_max_sp_max_sim_card_set': [], 'mAUROC_px_sim_card_set': [], 'mAP_px_sim_card_set': [], 'mF1_max_px_sim_card_set': [], 'mAUPRO_px_sim_card_set': [], 'mIoU_max_px_sim_card_set': [], 'S_AUROC_switch': [], 'S_AUPR_switch': [], 'mAUROC_sp_max_switch': [], 'mAP_sp_max_switch': [], 'mF1_max_sp_max_switch': [], 'mAUROC_px_switch': [], 'mAP_px_switch': [], 'mF1_max_px_switch': [], 'mAUPRO_px_switch': [], 'mIoU_max_px_switch': [], 'S_AUROC_tape': [], 'S_AUPR_tape': [], 'mAUROC_sp_max_tape': [], 'mAP_sp_max_tape': [], 'mF1_max_sp_max_tape': [], 'mAUROC_px_tape': [], 'mAP_px_tape': [], 'mF1_max_px_tape': [], 'mAUPRO_px_tape': [], 'mIoU_max_px_tape': [], 'S_AUROC_terminalblock': [], 'S_AUPR_terminalblock': [], 'mAUROC_sp_max_terminalblock': [], 'mAP_sp_max_terminalblock': [], 'mF1_max_sp_max_terminalblock': [], 'mAUROC_px_terminalblock': [], 'mAP_px_terminalblock': [], 'mF1_max_px_terminalblock': [], 'mAUPRO_px_terminalblock': [], 'mIoU_max_px_terminalblock': [], 'S_AUROC_toothbrush': [], 'S_AUPR_toothbrush': [], 'mAUROC_sp_max_toothbrush': [], 'mAP_sp_max_toothbrush': [], 'mF1_max_sp_max_toothbrush': [], 'mAUROC_px_toothbrush': [], 'mAP_px_toothbrush': [], 'mF1_max_px_toothbrush': [], 'mAUPRO_px_toothbrush': [], 'mIoU_max_px_toothbrush': [], 'S_AUROC_toy': [], 'S_AUPR_toy': [], 'mAUROC_sp_max_toy': [], 'mAP_sp_max_toy': [], 'mF1_max_sp_max_toy': [], 'mAUROC_px_toy': [], 'mAP_px_toy': [], 'mF1_max_px_toy': [], 'mAUPRO_px_toy': [], 'mIoU_max_px_toy': [], 'S_AUROC_toy_brick': [], 'S_AUPR_toy_brick': [], 'mAUROC_sp_max_toy_brick': [], 'mAP_sp_max_toy_brick': [], 'mF1_max_sp_max_toy_brick': [], 'mAUROC_px_toy_brick': [], 'mAP_px_toy_brick': [], 'mF1_max_px_toy_brick': [], 'mAUPRO_px_toy_brick': [], 'mIoU_max_px_toy_brick': [], 'S_AUROC_transistor1': [], 'S_AUPR_transistor1': [], 'mAUROC_sp_max_transistor1': [], 'mAP_sp_max_transistor1': [], 'mF1_max_sp_max_transistor1': [], 'mAUROC_px_transistor1': [], 'mAP_px_transistor1': [], 'mF1_max_px_transistor1': [], 'mAUPRO_px_transistor1': [], 'mIoU_max_px_transistor1': [], 'S_AUROC_u_block': [], 'S_AUPR_u_block': [], 'mAUROC_sp_max_u_block': [], 'mAP_sp_max_u_block': [], 'mF1_max_sp_max_u_block': [], 'mAUROC_px_u_block': [], 'mAP_px_u_block': [], 'mF1_max_px_u_block': [], 'mAUPRO_px_u_block': [], 'mIoU_max_px_u_block': [], 'S_AUROC_usb': [], 'S_AUPR_usb': [], 'mAUROC_sp_max_usb': [], 'mAP_sp_max_usb': [], 'mF1_max_sp_max_usb': [], 'mAUROC_px_usb': [], 'mAP_px_usb': [], 'mF1_max_px_usb': [], 'mAUPRO_px_usb': [], 'mIoU_max_px_usb': [], 'S_AUROC_usb_adaptor': [], 'S_AUPR_usb_adaptor': [], 'mAUROC_sp_max_usb_adaptor': [], 'mAP_sp_max_usb_adaptor': [], 'mF1_max_sp_max_usb_adaptor': [], 'mAUROC_px_usb_adaptor': [], 'mAP_px_usb_adaptor': [], 'mF1_max_px_usb_adaptor': [], 'mAUPRO_px_usb_adaptor': [], 'mIoU_max_px_usb_adaptor': [], 'S_AUROC_vcpill': [], 'S_AUPR_vcpill': [], 'mAUROC_sp_max_vcpill': [], 'mAP_sp_max_vcpill': [], 'mF1_max_sp_max_vcpill': [], 'mAUROC_px_vcpill': [], 'mAP_px_vcpill': [], 'mF1_max_px_vcpill': [], 'mAUPRO_px_vcpill': [], 'mIoU_max_px_vcpill': [], 'S_AUROC_wooden_beads': [], 'S_AUPR_wooden_beads': [], 'mAUROC_sp_max_wooden_beads': [], 'mAP_sp_max_wooden_beads': [], 'mF1_max_sp_max_wooden_beads': [], 'mAUROC_px_wooden_beads': [], 'mAP_px_wooden_beads': [], 'mF1_max_px_wooden_beads': [], 'mAUPRO_px_wooden_beads': [], 'mIoU_max_px_wooden_beads': [], 'S_AUROC_woodstick': [], 'S_AUPR_woodstick': [], 'mAUROC_sp_max_woodstick': [], 'mAP_sp_max_woodstick': [], 'mF1_max_sp_max_woodstick': [], 'mAUROC_px_woodstick': [], 'mAP_px_woodstick': [], 'mF1_max_px_woodstick': [], 'mAUPRO_px_woodstick': [], 'mIoU_max_px_woodstick': [], 'S_AUROC_zipper': [], 'S_AUROC_Avg': [], 'S_AUPR_zipper': [], 'S_AUPR_Avg': [], 'mAUROC_sp_max_zipper': [], 'mAUROC_sp_max_Avg': [], 'mAP_sp_max_zipper': [], 'mAP_sp_max_Avg': [], 'mF1_max_sp_max_zipper': [], 'mF1_max_sp_max_Avg': [], 'mAUROC_px_zipper': [], 'mAUROC_px_Avg': [], 'mAP_px_zipper': [], 'mAP_px_Avg': [], 'mF1_max_px_zipper': [], 'mF1_max_px_Avg': [], 'mAUPRO_px_zipper': [], 'mAUPRO_px_Avg': [], 'mIoU_max_px_zipper': [], 'mIoU_max_px_Avg': []}
loss.loss_terms                      : [{'type': 'CosLoss', 'name': 'cos', 'avg': False, 'lam': 1.0}]
loss.clip_grad                       : None                                
loss.create_graph                    : False                               
loss.retain_graph                    : False                               
adv                                  : False                               
logging.log_terms_train              : [{'name': 'batch_t', 'fmt': ':>5.3f', 'add_name': 'avg'}, {'name': 'data_t', 'fmt': ':>5.3f'}, {'name': 'optim_t', 'fmt': ':>5.3f'}, {'name': 'lr', 'fmt': ':>7.6f'}, {'name': 'pixel', 'suffixes': [''], 'fmt': ':>5.3f', 'add_name': 'avg'}]
logging.log_terms_test               : [{'name': 'batch_t', 'fmt': ':>5.3f', 'add_name': 'avg'}, {'name': 'pixel', 'suffixes': [''], 'fmt': ':>5.3f', 'add_name': 'avg'}]
logging.train_reset_log_per          : 50                                  
logging.train_log_per                : 50                                  
logging.test_log_per                 : 50                                  
data.sampler                         : naive                               
data.loader_type                     : pil                                 
data.loader_type_target              : pil_L                               
data.type                            : RealIAD                             
data.root                            : /home/albus/DataSets/REAL-IAD/realiad_256
data.meta                            : meta.json                           
data.cls_names                       : []                                  
data.train_transforms                : [{'type': 'Resize', 'size': (256, 256), 'interpolation': <InterpolationMode.BILINEAR: 'bilinear'>}, {'type': 'CenterCrop', 'size': (256, 256)}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'inplace': True}]
data.test_transforms                 : [{'type': 'Resize', 'size': (256, 256), 'interpolation': <InterpolationMode.BILINEAR: 'bilinear'>}, {'type': 'CenterCrop', 'size': (256, 256)}, {'type': 'ToTensor'}, {'type': 'Normalize', 'mean': (0.485, 0.456, 0.406), 'std': (0.229, 0.224, 0.225), 'inplace': True}]
data.target_transforms               : [{'type': 'Resize', 'size': (256, 256), 'interpolation': <InterpolationMode.BILINEAR: 'bilinear'>}, {'type': 'CenterCrop', 'size': (256, 256)}, {'type': 'ToTensor'}]
data.use_sample                      : True                                
data.views                           : []                                  
data.train_size                      : 911                                 
data.test_size                       : 2865                                
data.train_length                    : 7293                                
data.test_length                     : 22917                               
model_backbone.name                  : timm_wide_resnet50_2                
model_backbone.device                : cuda                                
model_backbone.dec_arch              : freia-cflow                         
model_backbone.condition_vec         : 128                                 
model_backbone.coupling_blocks       : 8                                   
model_backbone.clamp_alpha           : 1.9                                 
model_backbone.kwargs                : {'pretrained': True, 'checkpoint_path': '', 'strict': False, 'features_only': True, 'out_indices': [1, 2, 3, 4]}
model.name                           : cflow                               
model.pool_layers                    : 3                                   
model.N                              : 256                                 
model.kwargs                         : {'pretrained': True, 'checkpoint_path': '', 'strict': True, 'model_backbone': Namespace(clamp_alpha=1.9, condition_vec=128, coupling_blocks=8, dec_arch='freia-cflow', device='cuda', kwargs={'pretrained': True, 'checkpoint_path': '', 'strict': False, 'features_only': True, 'out_indices': [1, 2, 3, 4]}, name='timm_wide_resnet50_2'), 'L': 3, 'N': 256}
seed                                 : 42                                  
size                                 : 256                                 
warmup_epochs                        : 0                                   
test_start_epoch                     : 100                                 
test_per_epoch                       : 10                                  
batch_train                          : 8                                   
batch_test_per                       : 8                                   
lr                                   : 0.0001                              
weight_decay                         : 0.0001                              
cfg_path                             : configs.z_realiad.cflow_256_100e    
mode                                 : train                               
sleep                                : -1                                  
memory                               : -1                                  
dist_url                             : env://                              
logger_rank                          : 0                                   
opts                                 : []                                  
command                              : python3 -m torch.distributed.launch --nproc_per_node=$nproc_per_node --nnodes=$nnodes --node_rank=$node_rank --master_addr=$master_addr --master_port=$master_port --use_env run.py -c configs.z_realiad.cflow_256_100e -m train --sleep -1 --memory -1 --dist_url env:// --logger_rank 0 
task_start_time                      : 272509.101791095                    
dist                                 : False                               
world_size                           : 1                                   
rank                                 : 0                                   
local_rank                           : 0                                   
ngpus_per_node                       : 1                                   
nnodes                               : 1                                   
master                               : True                                
logdir                               : runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245
logger.filters                       : []                                  
logger.name                          : root                                
logger.level                         : 20                                  
logger.parent                        : None                                
logger.propagate                     : True                                
logger.disabled                      : False                               
logdir_train                         : runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245/show_train
logdir_test                          : runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245/show_test
2025-04-26 20:43:57,419 - ==> Starting training with 1 nodes x 1 GPUs
2025-04-26 20:45:36,619 - Train: 0.05% [50/91100]  [0.1/100.0] [batch_t 1.958 (1.981)] [data_t 0.004] [optim_t 1.954] [pixel 0.755 (1.011)]
2025-04-26 20:47:14,775 - Train: 0.11% [100/91100]  [0.1/100.0] [batch_t 1.964 (1.963)] [data_t 0.003] [optim_t 1.961] [pixel 0.749 (0.719)]
2025-04-26 20:48:53,017 - Train: 0.16% [150/91100]  [0.2/100.0] [batch_t 1.966 (1.965)] [data_t 0.004] [optim_t 1.962] [pixel 0.595 (0.656)]
2025-04-26 20:50:31,383 - Train: 0.22% [200/91100]  [0.2/100.0] [batch_t 1.965 (1.967)] [data_t 0.004] [optim_t 1.962] [pixel 0.622 (0.620)]
2025-04-26 20:52:09,657 - Train: 0.27% [250/91100]  [0.3/100.0] [batch_t 1.969 (1.965)] [data_t 0.004] [optim_t 1.965] [pixel 0.619 (0.603)]
2025-04-26 20:53:47,741 - Train: 0.33% [300/91100]  [0.3/100.0] [batch_t 1.962 (1.962)] [data_t 0.003] [optim_t 1.959] [pixel 0.539 (0.583)]
2025-04-26 20:55:25,797 - Train: 0.38% [350/91100]  [0.4/100.0] [batch_t 1.974 (1.961)] [data_t 0.004] [optim_t 1.970] [pixel 0.586 (0.572)]
2025-04-26 20:57:03,452 - Train: 0.44% [400/91100]  [0.4/100.0] [batch_t 1.953 (1.953)] [data_t 0.003] [optim_t 1.950] [pixel 0.548 (0.552)]
2025-04-26 20:58:41,185 - Train: 0.49% [450/91100]  [0.5/100.0] [batch_t 1.955 (1.955)] [data_t 0.003] [optim_t 1.951] [pixel 0.555 (0.544)]
2025-04-26 21:00:18,902 - Train: 0.55% [500/91100]  [0.5/100.0] [batch_t 1.954 (1.954)] [data_t 0.004] [optim_t 1.951] [pixel 0.509 (0.536)]
2025-04-26 21:01:56,682 - Train: 0.60% [550/91100]  [0.6/100.0] [batch_t 1.952 (1.956)] [data_t 0.004] [optim_t 1.949] [pixel 0.593 (0.523)]
2025-04-26 21:03:34,424 - Train: 0.66% [600/91100]  [0.7/100.0] [batch_t 1.956 (1.955)] [data_t 0.004] [optim_t 1.952] [pixel 0.410 (0.513)]
2025-04-26 21:05:12,184 - Train: 0.71% [650/91100]  [0.7/100.0] [batch_t 1.951 (1.955)] [data_t 0.004] [optim_t 1.947] [pixel 0.545 (0.511)]
2025-04-26 21:06:49,910 - Train: 0.77% [700/91100]  [0.8/100.0] [batch_t 1.956 (1.954)] [data_t 0.004] [optim_t 1.952] [pixel 0.434 (0.492)]
2025-04-26 21:08:27,585 - Train: 0.82% [750/91100]  [0.8/100.0] [batch_t 1.952 (1.953)] [data_t 0.004] [optim_t 1.948] [pixel 0.519 (0.501)]
2025-04-26 21:10:05,178 - Train: 0.88% [800/91100]  [0.9/100.0] [batch_t 1.953 (1.952)] [data_t 0.003] [optim_t 1.949] [pixel 0.536 (0.489)]
2025-04-26 21:11:42,175 - Train: 0.93% [850/91100]  [0.9/100.0] [batch_t 1.954 (1.940)] [data_t 0.003] [optim_t 1.951] [pixel 0.449 (0.473)]
2025-04-26 21:13:19,654 - Train: 0.99% [900/91100]  [1.0/100.0] [batch_t 1.945 (1.950)] [data_t 0.003] [optim_t 1.941] [pixel 0.477 (0.474)]
2025-04-26 21:13:41,063 - ==> Total time: 0:30:55	 Eta: 4:38:22 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 21:14:58,783 - Train: 1.04% [950/91100]  [1.0/100.0] [batch_t 1.949 (1.973)] [data_t 0.003] [optim_t 1.945] [pixel 0.441 (0.454)]
2025-04-26 21:16:36,313 - Train: 1.10% [1000/91100]  [1.1/100.0] [batch_t 1.947 (1.951)] [data_t 0.004] [optim_t 1.943] [pixel 0.439 (0.461)]
2025-04-26 21:18:13,881 - Train: 1.15% [1050/91100]  [1.2/100.0] [batch_t 1.950 (1.951)] [data_t 0.004] [optim_t 1.946] [pixel 0.454 (0.445)]
2025-04-26 21:19:51,440 - Train: 1.21% [1100/91100]  [1.2/100.0] [batch_t 1.949 (1.951)] [data_t 0.003] [optim_t 1.945] [pixel 0.514 (0.450)]
2025-04-26 21:21:29,080 - Train: 1.26% [1150/91100]  [1.3/100.0] [batch_t 1.969 (1.953)] [data_t 0.004] [optim_t 1.965] [pixel 0.495 (0.443)]
2025-04-26 21:23:06,981 - Train: 1.32% [1200/91100]  [1.3/100.0] [batch_t 1.952 (1.958)] [data_t 0.003] [optim_t 1.949] [pixel 0.441 (0.438)]
2025-04-26 21:24:44,711 - Train: 1.37% [1250/91100]  [1.4/100.0] [batch_t 1.954 (1.955)] [data_t 0.004] [optim_t 1.950] [pixel 0.369 (0.438)]
2025-04-26 21:26:22,529 - Train: 1.43% [1300/91100]  [1.4/100.0] [batch_t 1.951 (1.956)] [data_t 0.004] [optim_t 1.948] [pixel 0.381 (0.430)]
2025-04-26 21:28:00,137 - Train: 1.48% [1350/91100]  [1.5/100.0] [batch_t 1.954 (1.952)] [data_t 0.003] [optim_t 1.951] [pixel 0.411 (0.424)]
2025-04-26 21:29:37,933 - Train: 1.54% [1400/91100]  [1.5/100.0] [batch_t 1.954 (1.956)] [data_t 0.004] [optim_t 1.951] [pixel 0.404 (0.430)]
2025-04-26 21:31:15,626 - Train: 1.59% [1450/91100]  [1.6/100.0] [batch_t 1.983 (1.954)] [data_t 0.004] [optim_t 1.980] [pixel 0.412 (0.416)]
2025-04-26 21:32:53,462 - Train: 1.65% [1500/91100]  [1.6/100.0] [batch_t 1.954 (1.957)] [data_t 0.003] [optim_t 1.951] [pixel 0.392 (0.411)]
2025-04-26 21:34:30,979 - Train: 1.70% [1550/91100]  [1.7/100.0] [batch_t 1.953 (1.950)] [data_t 0.003] [optim_t 1.949] [pixel 0.396 (0.414)]
2025-04-26 21:36:07,718 - Train: 1.76% [1600/91100]  [1.8/100.0] [batch_t 1.919 (1.935)] [data_t 0.004] [optim_t 1.915] [pixel 0.426 (0.411)]
2025-04-26 21:37:43,945 - Train: 1.81% [1650/91100]  [1.8/100.0] [batch_t 1.919 (1.924)] [data_t 0.003] [optim_t 1.915] [pixel 0.414 (0.405)]
2025-04-26 21:39:20,098 - Train: 1.87% [1700/91100]  [1.9/100.0] [batch_t 1.955 (1.923)] [data_t 0.004] [optim_t 1.952] [pixel 0.397 (0.407)]
2025-04-26 21:40:58,758 - Train: 1.92% [1750/91100]  [1.9/100.0] [batch_t 1.971 (1.973)] [data_t 0.003] [optim_t 1.967] [pixel 0.401 (0.404)]
2025-04-26 21:42:37,321 - Train: 1.98% [1800/91100]  [2.0/100.0] [batch_t 1.972 (1.971)] [data_t 0.003] [optim_t 1.969] [pixel 0.400 (0.393)]
2025-04-26 21:43:20,718 - ==> Total time: 1:00:35	 Eta: 4:02:22 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 21:44:16,891 - Train: 2.03% [1850/91100]  [2.0/100.0] [batch_t 1.941 (1.967)] [data_t 0.003] [optim_t 1.938] [pixel 0.385 (0.384)]
2025-04-26 21:45:54,942 - Train: 2.09% [1900/91100]  [2.1/100.0] [batch_t 1.990 (1.961)] [data_t 0.003] [optim_t 1.987] [pixel 0.405 (0.390)]
2025-04-26 21:47:34,212 - Train: 2.14% [1950/91100]  [2.1/100.0] [batch_t 1.980 (1.985)] [data_t 0.004] [optim_t 1.976] [pixel 0.390 (0.384)]
2025-04-26 21:49:13,116 - Train: 2.20% [2000/91100]  [2.2/100.0] [batch_t 1.975 (1.978)] [data_t 0.004] [optim_t 1.972] [pixel 0.406 (0.382)]
2025-04-26 21:50:52,250 - Train: 2.25% [2050/91100]  [2.3/100.0] [batch_t 1.983 (1.983)] [data_t 0.004] [optim_t 1.980] [pixel 0.395 (0.384)]
2025-04-26 21:52:31,485 - Train: 2.31% [2100/91100]  [2.3/100.0] [batch_t 1.988 (1.985)] [data_t 0.004] [optim_t 1.985] [pixel 0.398 (0.382)]
2025-04-26 21:54:10,836 - Train: 2.36% [2150/91100]  [2.4/100.0] [batch_t 1.987 (1.987)] [data_t 0.004] [optim_t 1.983] [pixel 0.388 (0.375)]
2025-04-26 21:55:50,143 - Train: 2.41% [2200/91100]  [2.4/100.0] [batch_t 1.994 (1.986)] [data_t 0.004] [optim_t 1.990] [pixel 0.377 (0.370)]
2025-04-26 21:57:29,378 - Train: 2.47% [2250/91100]  [2.5/100.0] [batch_t 1.968 (1.985)] [data_t 0.004] [optim_t 1.964] [pixel 0.371 (0.370)]
2025-04-26 21:59:08,504 - Train: 2.52% [2300/91100]  [2.5/100.0] [batch_t 1.986 (1.982)] [data_t 0.004] [optim_t 1.982] [pixel 0.388 (0.359)]
2025-04-26 22:00:47,935 - Train: 2.58% [2350/91100]  [2.6/100.0] [batch_t 1.993 (1.989)] [data_t 0.004] [optim_t 1.989] [pixel 0.376 (0.365)]
2025-04-26 22:02:27,943 - Train: 2.63% [2400/91100]  [2.6/100.0] [batch_t 2.001 (2.000)] [data_t 0.004] [optim_t 1.998] [pixel 0.364 (0.363)]
2025-04-26 22:04:07,452 - Train: 2.69% [2450/91100]  [2.7/100.0] [batch_t 1.988 (1.990)] [data_t 0.004] [optim_t 1.985] [pixel 0.320 (0.360)]
2025-04-26 22:05:45,143 - Train: 2.74% [2500/91100]  [2.7/100.0] [batch_t 1.955 (1.954)] [data_t 0.004] [optim_t 1.951] [pixel 0.343 (0.354)]
2025-04-26 22:07:22,869 - Train: 2.80% [2550/91100]  [2.8/100.0] [batch_t 1.939 (1.954)] [data_t 0.004] [optim_t 1.935] [pixel 0.370 (0.357)]
2025-04-26 22:09:01,344 - Train: 2.85% [2600/91100]  [2.9/100.0] [batch_t 1.957 (1.969)] [data_t 0.004] [optim_t 1.954] [pixel 0.398 (0.351)]
2025-04-26 22:10:39,920 - Train: 2.91% [2650/91100]  [2.9/100.0] [batch_t 1.960 (1.971)] [data_t 0.004] [optim_t 1.956] [pixel 0.369 (0.359)]
2025-04-26 22:12:17,956 - Train: 2.96% [2700/91100]  [3.0/100.0] [batch_t 1.972 (1.961)] [data_t 0.003] [optim_t 1.969] [pixel 0.393 (0.350)]
2025-04-26 22:13:22,604 - ==> Total time: 1:30:37	 Eta: 3:31:27 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 22:13:57,367 - Train: 3.02% [2750/91100]  [3.0/100.0] [batch_t 1.950 (1.959)] [data_t 0.004] [optim_t 1.947] [pixel 0.329 (0.348)]
2025-04-26 22:15:34,853 - Train: 3.07% [2800/91100]  [3.1/100.0] [batch_t 1.948 (1.950)] [data_t 0.004] [optim_t 1.945] [pixel 0.324 (0.349)]
2025-04-26 22:17:13,309 - Train: 3.13% [2850/91100]  [3.1/100.0] [batch_t 1.957 (1.969)] [data_t 0.004] [optim_t 1.953] [pixel 0.305 (0.344)]
2025-04-26 22:18:51,164 - Train: 3.18% [2900/91100]  [3.2/100.0] [batch_t 1.949 (1.957)] [data_t 0.004] [optim_t 1.945] [pixel 0.343 (0.342)]
2025-04-26 22:20:28,651 - Train: 3.24% [2950/91100]  [3.2/100.0] [batch_t 1.951 (1.950)] [data_t 0.004] [optim_t 1.947] [pixel 0.343 (0.343)]
2025-04-26 22:22:06,172 - Train: 3.29% [3000/91100]  [3.3/100.0] [batch_t 1.953 (1.950)] [data_t 0.004] [optim_t 1.949] [pixel 0.321 (0.341)]
2025-04-26 22:23:45,392 - Train: 3.35% [3050/91100]  [3.3/100.0] [batch_t 1.994 (1.984)] [data_t 0.003] [optim_t 1.990] [pixel 0.328 (0.336)]
2025-04-26 22:25:23,997 - Train: 3.40% [3100/91100]  [3.4/100.0] [batch_t 1.948 (1.972)] [data_t 0.004] [optim_t 1.944] [pixel 0.402 (0.343)]
2025-04-26 22:27:02,852 - Train: 3.46% [3150/91100]  [3.5/100.0] [batch_t 1.940 (1.977)] [data_t 0.004] [optim_t 1.937] [pixel 0.401 (0.336)]
2025-04-26 22:28:41,022 - Train: 3.51% [3200/91100]  [3.5/100.0] [batch_t 1.991 (1.963)] [data_t 0.004] [optim_t 1.988] [pixel 0.310 (0.335)]
2025-04-26 22:30:20,526 - Train: 3.57% [3250/91100]  [3.6/100.0] [batch_t 1.989 (1.990)] [data_t 0.004] [optim_t 1.985] [pixel 0.297 (0.334)]
2025-04-26 22:32:00,290 - Train: 3.62% [3300/91100]  [3.6/100.0] [batch_t 1.968 (1.995)] [data_t 0.004] [optim_t 1.965] [pixel 0.346 (0.326)]
2025-04-26 22:33:40,391 - Train: 3.68% [3350/91100]  [3.7/100.0] [batch_t 2.006 (2.002)] [data_t 0.004] [optim_t 2.002] [pixel 0.318 (0.327)]
2025-04-26 22:35:19,704 - Train: 3.73% [3400/91100]  [3.7/100.0] [batch_t 1.986 (1.986)] [data_t 0.004] [optim_t 1.982] [pixel 0.318 (0.330)]
2025-04-26 22:36:59,105 - Train: 3.79% [3450/91100]  [3.8/100.0] [batch_t 1.989 (1.988)] [data_t 0.004] [optim_t 1.985] [pixel 0.353 (0.326)]
2025-04-26 22:38:38,440 - Train: 3.84% [3500/91100]  [3.8/100.0] [batch_t 1.984 (1.987)] [data_t 0.004] [optim_t 1.981] [pixel 0.347 (0.330)]
2025-04-26 22:40:17,840 - Train: 3.90% [3550/91100]  [3.9/100.0] [batch_t 1.984 (1.988)] [data_t 0.003] [optim_t 1.980] [pixel 0.314 (0.318)]
2025-04-26 22:41:57,181 - Train: 3.95% [3600/91100]  [4.0/100.0] [batch_t 1.985 (1.987)] [data_t 0.004] [optim_t 1.982] [pixel 0.292 (0.325)]
2025-04-26 22:43:25,165 - ==> Total time: 2:00:39	 Eta: 3:00:59 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 22:43:38,419 - Train: 4.01% [3650/91100]  [4.0/100.0] [batch_t 1.987 (2.024)] [data_t 0.004] [optim_t 1.983] [pixel 0.329 (0.318)]
2025-04-26 22:45:18,412 - Train: 4.06% [3700/91100]  [4.1/100.0] [batch_t 2.014 (2.000)] [data_t 0.004] [optim_t 2.010] [pixel 0.392 (0.312)]
2025-04-26 22:46:58,203 - Train: 4.12% [3750/91100]  [4.1/100.0] [batch_t 1.988 (1.996)] [data_t 0.004] [optim_t 1.985] [pixel 0.358 (0.317)]
2025-04-26 22:48:37,018 - Train: 4.17% [3800/91100]  [4.2/100.0] [batch_t 1.954 (1.976)] [data_t 0.004] [optim_t 1.951] [pixel 0.384 (0.316)]
2025-04-26 22:50:13,730 - Train: 4.23% [3850/91100]  [4.2/100.0] [batch_t 1.917 (1.934)] [data_t 0.003] [optim_t 1.913] [pixel 0.311 (0.311)]
2025-04-26 22:51:50,628 - Train: 4.28% [3900/91100]  [4.3/100.0] [batch_t 1.941 (1.938)] [data_t 0.004] [optim_t 1.937] [pixel 0.293 (0.316)]
2025-04-26 22:53:27,730 - Train: 4.34% [3950/91100]  [4.3/100.0] [batch_t 1.939 (1.942)] [data_t 0.004] [optim_t 1.935] [pixel 0.303 (0.318)]
2025-04-26 22:55:04,897 - Train: 4.39% [4000/91100]  [4.4/100.0] [batch_t 1.941 (1.943)] [data_t 0.004] [optim_t 1.937] [pixel 0.293 (0.309)]
2025-04-26 22:56:42,752 - Train: 4.45% [4050/91100]  [4.4/100.0] [batch_t 1.957 (1.957)] [data_t 0.004] [optim_t 1.953] [pixel 0.342 (0.310)]
2025-04-26 22:58:20,543 - Train: 4.50% [4100/91100]  [4.5/100.0] [batch_t 1.954 (1.956)] [data_t 0.004] [optim_t 1.950] [pixel 0.299 (0.310)]
2025-04-26 22:59:58,329 - Train: 4.56% [4150/91100]  [4.6/100.0] [batch_t 1.953 (1.956)] [data_t 0.004] [optim_t 1.950] [pixel 0.309 (0.317)]
2025-04-26 23:01:36,126 - Train: 4.61% [4200/91100]  [4.6/100.0] [batch_t 1.955 (1.956)] [data_t 0.004] [optim_t 1.952] [pixel 0.278 (0.305)]
2025-04-26 23:03:13,919 - Train: 4.67% [4250/91100]  [4.7/100.0] [batch_t 1.956 (1.956)] [data_t 0.004] [optim_t 1.953] [pixel 0.345 (0.308)]
2025-04-26 23:04:51,302 - Train: 4.72% [4300/91100]  [4.7/100.0] [batch_t 1.940 (1.948)] [data_t 0.004] [optim_t 1.937] [pixel 0.326 (0.308)]
2025-04-26 23:06:28,351 - Train: 4.77% [4350/91100]  [4.8/100.0] [batch_t 1.939 (1.941)] [data_t 0.004] [optim_t 1.935] [pixel 0.263 (0.308)]
2025-04-26 23:08:05,377 - Train: 4.83% [4400/91100]  [4.8/100.0] [batch_t 1.944 (1.940)] [data_t 0.003] [optim_t 1.941] [pixel 0.344 (0.304)]
2025-04-26 23:09:42,657 - Train: 4.88% [4450/91100]  [4.9/100.0] [batch_t 1.946 (1.946)] [data_t 0.003] [optim_t 1.943] [pixel 0.307 (0.309)]
2025-04-26 23:11:19,913 - Train: 4.94% [4500/91100]  [4.9/100.0] [batch_t 1.956 (1.945)] [data_t 0.004] [optim_t 1.953] [pixel 0.308 (0.301)]
2025-04-26 23:12:57,699 - Train: 4.99% [4550/91100]  [5.0/100.0] [batch_t 1.958 (1.956)] [data_t 0.003] [optim_t 1.954] [pixel 0.330 (0.305)]
2025-04-26 23:13:07,475 - ==> Total time: 2:30:22	 Eta: 2:30:22 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 23:14:36,496 - Train: 5.05% [4600/91100]  [5.0/100.0] [batch_t 1.945 (1.953)] [data_t 0.003] [optim_t 1.941] [pixel 0.312 (0.301)]
2025-04-26 23:16:14,161 - Train: 5.10% [4650/91100]  [5.1/100.0] [batch_t 1.967 (1.953)] [data_t 0.004] [optim_t 1.964] [pixel 0.268 (0.302)]
2025-04-26 23:17:52,020 - Train: 5.16% [4700/91100]  [5.2/100.0] [batch_t 1.956 (1.957)] [data_t 0.003] [optim_t 1.952] [pixel 0.313 (0.294)]
2025-04-26 23:19:31,301 - Train: 5.21% [4750/91100]  [5.2/100.0] [batch_t 1.996 (1.986)] [data_t 0.004] [optim_t 1.993] [pixel 0.308 (0.300)]
2025-04-26 23:21:10,658 - Train: 5.27% [4800/91100]  [5.3/100.0] [batch_t 1.986 (1.987)] [data_t 0.003] [optim_t 1.983] [pixel 0.283 (0.294)]
2025-04-26 23:22:50,065 - Train: 5.32% [4850/91100]  [5.3/100.0] [batch_t 1.990 (1.988)] [data_t 0.004] [optim_t 1.986] [pixel 0.266 (0.298)]
2025-04-26 23:24:29,528 - Train: 5.38% [4900/91100]  [5.4/100.0] [batch_t 1.989 (1.989)] [data_t 0.004] [optim_t 1.985] [pixel 0.274 (0.297)]
2025-04-26 23:26:08,997 - Train: 5.43% [4950/91100]  [5.4/100.0] [batch_t 1.988 (1.989)] [data_t 0.004] [optim_t 1.985] [pixel 0.280 (0.291)]
2025-04-26 23:27:48,571 - Train: 5.49% [5000/91100]  [5.5/100.0] [batch_t 1.986 (1.991)] [data_t 0.003] [optim_t 1.982] [pixel 0.298 (0.293)]
2025-04-26 23:29:27,860 - Train: 5.54% [5050/91100]  [5.5/100.0] [batch_t 1.987 (1.986)] [data_t 0.004] [optim_t 1.983] [pixel 0.285 (0.295)]
2025-04-26 23:31:05,867 - Train: 5.60% [5100/91100]  [5.6/100.0] [batch_t 1.950 (1.960)] [data_t 0.004] [optim_t 1.946] [pixel 0.303 (0.292)]
2025-04-26 23:32:43,634 - Train: 5.65% [5150/91100]  [5.7/100.0] [batch_t 1.949 (1.955)] [data_t 0.004] [optim_t 1.945] [pixel 0.317 (0.288)]
2025-04-26 23:34:21,204 - Train: 5.71% [5200/91100]  [5.7/100.0] [batch_t 1.982 (1.951)] [data_t 0.003] [optim_t 1.979] [pixel 0.263 (0.289)]
2025-04-26 23:35:58,799 - Train: 5.76% [5250/91100]  [5.8/100.0] [batch_t 1.946 (1.952)] [data_t 0.004] [optim_t 1.943] [pixel 0.289 (0.293)]
2025-04-26 23:37:36,669 - Train: 5.82% [5300/91100]  [5.8/100.0] [batch_t 1.973 (1.957)] [data_t 0.004] [optim_t 1.970] [pixel 0.310 (0.287)]
2025-04-26 23:39:14,429 - Train: 5.87% [5350/91100]  [5.9/100.0] [batch_t 1.955 (1.955)] [data_t 0.003] [optim_t 1.952] [pixel 0.297 (0.289)]
2025-04-26 23:40:52,380 - Train: 5.93% [5400/91100]  [5.9/100.0] [batch_t 1.959 (1.959)] [data_t 0.004] [optim_t 1.955] [pixel 0.315 (0.282)]
2025-04-26 23:42:29,790 - Train: 5.98% [5450/91100]  [6.0/100.0] [batch_t 1.948 (1.948)] [data_t 0.003] [optim_t 1.945] [pixel 0.299 (0.290)]
2025-04-26 23:43:00,966 - ==> Total time: 3:00:15	 Eta: 2:00:10 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-26 23:44:09,727 - Train: 6.04% [5500/91100]  [6.0/100.0] [batch_t 1.993 (1.990)] [data_t 0.004] [optim_t 1.990] [pixel 0.305 (0.285)]
2025-04-26 23:45:47,757 - Train: 6.09% [5550/91100]  [6.1/100.0] [batch_t 1.950 (1.961)] [data_t 0.004] [optim_t 1.947] [pixel 0.296 (0.288)]
2025-04-26 23:47:24,884 - Train: 6.15% [5600/91100]  [6.1/100.0] [batch_t 1.954 (1.943)] [data_t 0.004] [optim_t 1.951] [pixel 0.247 (0.281)]
2025-04-26 23:49:02,387 - Train: 6.20% [5650/91100]  [6.2/100.0] [batch_t 1.949 (1.950)] [data_t 0.004] [optim_t 1.945] [pixel 0.289 (0.286)]
2025-04-26 23:50:39,104 - Train: 6.26% [5700/91100]  [6.3/100.0] [batch_t 1.922 (1.934)] [data_t 0.004] [optim_t 1.918] [pixel 0.271 (0.279)]
2025-04-26 23:52:17,144 - Train: 6.31% [5750/91100]  [6.3/100.0] [batch_t 1.987 (1.961)] [data_t 0.004] [optim_t 1.983] [pixel 0.266 (0.279)]
2025-04-26 23:53:56,609 - Train: 6.37% [5800/91100]  [6.4/100.0] [batch_t 1.988 (1.989)] [data_t 0.004] [optim_t 1.985] [pixel 0.256 (0.285)]
2025-04-26 23:55:35,195 - Train: 6.42% [5850/91100]  [6.4/100.0] [batch_t 1.985 (1.972)] [data_t 0.003] [optim_t 1.982] [pixel 0.245 (0.284)]
2025-04-26 23:57:14,674 - Train: 6.48% [5900/91100]  [6.5/100.0] [batch_t 1.991 (1.990)] [data_t 0.003] [optim_t 1.988] [pixel 0.236 (0.279)]
2025-04-26 23:58:54,192 - Train: 6.53% [5950/91100]  [6.5/100.0] [batch_t 1.992 (1.990)] [data_t 0.004] [optim_t 1.988] [pixel 0.270 (0.281)]
2025-04-27 00:00:33,931 - Train: 6.59% [6000/91100]  [6.6/100.0] [batch_t 2.022 (1.995)] [data_t 0.004] [optim_t 2.018] [pixel 0.247 (0.273)]
2025-04-27 00:02:13,997 - Train: 6.64% [6050/91100]  [6.6/100.0] [batch_t 1.992 (2.001)] [data_t 0.004] [optim_t 1.988] [pixel 0.287 (0.280)]
2025-04-27 00:03:53,494 - Train: 6.70% [6100/91100]  [6.7/100.0] [batch_t 1.990 (1.990)] [data_t 0.004] [optim_t 1.987] [pixel 0.304 (0.275)]
2025-04-27 00:05:33,011 - Train: 6.75% [6150/91100]  [6.8/100.0] [batch_t 1.993 (1.990)] [data_t 0.003] [optim_t 1.989] [pixel 0.304 (0.275)]
2025-04-27 00:07:12,389 - Train: 6.81% [6200/91100]  [6.8/100.0] [batch_t 1.986 (1.988)] [data_t 0.003] [optim_t 1.982] [pixel 0.302 (0.272)]
2025-04-27 00:08:51,677 - Train: 6.86% [6250/91100]  [6.9/100.0] [batch_t 1.985 (1.986)] [data_t 0.003] [optim_t 1.981] [pixel 0.330 (0.274)]
2025-04-27 00:10:31,006 - Train: 6.92% [6300/91100]  [6.9/100.0] [batch_t 2.001 (1.987)] [data_t 0.004] [optim_t 1.997] [pixel 0.259 (0.276)]
2025-04-27 00:12:12,759 - Train: 6.97% [6350/91100]  [7.0/100.0] [batch_t 2.037 (2.035)] [data_t 0.004] [optim_t 2.034] [pixel 0.248 (0.269)]
2025-04-27 00:13:06,713 - ==> Total time: 3:30:21	 Eta: 1:30:09 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-27 00:13:54,887 - Train: 7.03% [6400/91100]  [7.0/100.0] [batch_t 2.037 (2.044)] [data_t 0.003] [optim_t 2.033] [pixel 0.240 (0.268)]
2025-04-27 00:15:35,091 - Train: 7.08% [6450/91100]  [7.1/100.0] [batch_t 1.989 (2.004)] [data_t 0.004] [optim_t 1.985] [pixel 0.259 (0.271)]
2025-04-27 00:17:14,894 - Train: 7.14% [6500/91100]  [7.1/100.0] [batch_t 1.986 (1.996)] [data_t 0.004] [optim_t 1.982] [pixel 0.284 (0.272)]
2025-04-27 00:18:53,931 - Train: 7.19% [6550/91100]  [7.2/100.0] [batch_t 1.978 (1.981)] [data_t 0.003] [optim_t 1.975] [pixel 0.285 (0.259)]
2025-04-27 00:20:33,345 - Train: 7.24% [6600/91100]  [7.2/100.0] [batch_t 1.997 (1.988)] [data_t 0.004] [optim_t 1.994] [pixel 0.212 (0.276)]
2025-04-27 00:22:12,681 - Train: 7.30% [6650/91100]  [7.3/100.0] [batch_t 1.983 (1.987)] [data_t 0.004] [optim_t 1.979] [pixel 0.279 (0.271)]
2025-04-27 00:23:51,738 - Train: 7.35% [6700/91100]  [7.4/100.0] [batch_t 1.978 (1.981)] [data_t 0.003] [optim_t 1.975] [pixel 0.307 (0.268)]
2025-04-27 00:25:30,801 - Train: 7.41% [6750/91100]  [7.4/100.0] [batch_t 1.981 (1.981)] [data_t 0.003] [optim_t 1.977] [pixel 0.267 (0.268)]
2025-04-27 00:27:09,867 - Train: 7.46% [6800/91100]  [7.5/100.0] [batch_t 1.980 (1.981)] [data_t 0.004] [optim_t 1.977] [pixel 0.296 (0.269)]
2025-04-27 00:28:48,912 - Train: 7.52% [6850/91100]  [7.5/100.0] [batch_t 1.983 (1.981)] [data_t 0.004] [optim_t 1.980] [pixel 0.315 (0.264)]
2025-04-27 00:30:28,054 - Train: 7.57% [6900/91100]  [7.6/100.0] [batch_t 1.989 (1.983)] [data_t 0.004] [optim_t 1.986] [pixel 0.245 (0.268)]
2025-04-27 00:32:07,246 - Train: 7.63% [6950/91100]  [7.6/100.0] [batch_t 1.959 (1.984)] [data_t 0.003] [optim_t 1.955] [pixel 0.287 (0.264)]
2025-04-27 00:33:45,150 - Train: 7.68% [7000/91100]  [7.7/100.0] [batch_t 1.955 (1.958)] [data_t 0.004] [optim_t 1.952] [pixel 0.245 (0.258)]
2025-04-27 00:35:23,008 - Train: 7.74% [7050/91100]  [7.7/100.0] [batch_t 1.956 (1.957)] [data_t 0.004] [optim_t 1.953] [pixel 0.246 (0.269)]
2025-04-27 00:37:01,275 - Train: 7.79% [7100/91100]  [7.8/100.0] [batch_t 1.985 (1.965)] [data_t 0.004] [optim_t 1.982] [pixel 0.288 (0.262)]
2025-04-27 00:38:40,400 - Train: 7.85% [7150/91100]  [7.8/100.0] [batch_t 1.990 (1.982)] [data_t 0.004] [optim_t 1.986] [pixel 0.269 (0.266)]
2025-04-27 00:40:19,508 - Train: 7.90% [7200/91100]  [7.9/100.0] [batch_t 1.986 (1.982)] [data_t 0.004] [optim_t 1.983] [pixel 0.299 (0.260)]
2025-04-27 00:41:58,643 - Train: 7.96% [7250/91100]  [8.0/100.0] [batch_t 1.981 (1.983)] [data_t 0.004] [optim_t 1.977] [pixel 0.244 (0.265)]
2025-04-27 00:43:13,766 - ==> Total time: 4:00:28	 Eta: 1:00:07 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-27 00:43:38,967 - Train: 8.01% [7300/91100]  [8.0/100.0] [batch_t 1.984 (2.008)] [data_t 0.003] [optim_t 1.980] [pixel 0.256 (0.259)]
2025-04-27 00:45:18,397 - Train: 8.07% [7350/91100]  [8.1/100.0] [batch_t 1.982 (1.989)] [data_t 0.004] [optim_t 1.979] [pixel 0.242 (0.258)]
2025-04-27 00:46:57,673 - Train: 8.12% [7400/91100]  [8.1/100.0] [batch_t 1.991 (1.985)] [data_t 0.004] [optim_t 1.987] [pixel 0.269 (0.258)]
2025-04-27 00:48:36,996 - Train: 8.18% [7450/91100]  [8.2/100.0] [batch_t 1.987 (1.986)] [data_t 0.004] [optim_t 1.983] [pixel 0.242 (0.263)]
2025-04-27 00:50:16,315 - Train: 8.23% [7500/91100]  [8.2/100.0] [batch_t 2.002 (1.986)] [data_t 0.004] [optim_t 1.999] [pixel 0.311 (0.254)]
2025-04-27 00:51:55,628 - Train: 8.29% [7550/91100]  [8.3/100.0] [batch_t 1.996 (1.986)] [data_t 0.004] [optim_t 1.993] [pixel 0.299 (0.258)]
2025-04-27 00:53:35,593 - Train: 8.34% [7600/91100]  [8.3/100.0] [batch_t 1.997 (1.999)] [data_t 0.004] [optim_t 1.994] [pixel 0.268 (0.262)]
2025-04-27 00:55:15,626 - Train: 8.40% [7650/91100]  [8.4/100.0] [batch_t 2.002 (2.001)] [data_t 0.004] [optim_t 1.998] [pixel 0.261 (0.254)]
2025-04-27 00:56:55,657 - Train: 8.45% [7700/91100]  [8.5/100.0] [batch_t 1.997 (2.001)] [data_t 0.004] [optim_t 1.993] [pixel 0.244 (0.262)]
2025-04-27 00:58:34,983 - Train: 8.51% [7750/91100]  [8.5/100.0] [batch_t 1.988 (1.986)] [data_t 0.004] [optim_t 1.985] [pixel 0.289 (0.253)]
2025-04-27 01:00:13,784 - Train: 8.56% [7800/91100]  [8.6/100.0] [batch_t 1.958 (1.976)] [data_t 0.004] [optim_t 1.955] [pixel 0.322 (0.260)]
2025-04-27 01:01:52,872 - Train: 8.62% [7850/91100]  [8.6/100.0] [batch_t 1.987 (1.982)] [data_t 0.004] [optim_t 1.984] [pixel 0.256 (0.257)]
2025-04-27 01:03:32,277 - Train: 8.67% [7900/91100]  [8.7/100.0] [batch_t 1.990 (1.988)] [data_t 0.004] [optim_t 1.987] [pixel 0.264 (0.253)]
2025-04-27 01:05:11,928 - Train: 8.73% [7950/91100]  [8.7/100.0] [batch_t 1.994 (1.993)] [data_t 0.003] [optim_t 1.991] [pixel 0.231 (0.250)]
2025-04-27 01:06:52,103 - Train: 8.78% [8000/91100]  [8.8/100.0] [batch_t 2.009 (2.003)] [data_t 0.004] [optim_t 2.005] [pixel 0.198 (0.250)]
2025-04-27 01:08:32,265 - Train: 8.84% [8050/91100]  [8.8/100.0] [batch_t 2.005 (2.003)] [data_t 0.004] [optim_t 2.001] [pixel 0.228 (0.251)]
2025-04-27 01:10:12,654 - Train: 8.89% [8100/91100]  [8.9/100.0] [batch_t 2.008 (2.008)] [data_t 0.004] [optim_t 2.005] [pixel 0.248 (0.251)]
2025-04-27 01:11:53,011 - Train: 8.95% [8150/91100]  [8.9/100.0] [batch_t 1.993 (2.007)] [data_t 0.004] [optim_t 1.989] [pixel 0.268 (0.259)]
2025-04-27 01:13:30,007 - ==> Total time: 4:30:44	 Eta: 0:30:04 	Logged in 'runs/CFLOWTrainer_configs_z_realiad_cflow_256_100e_20250426-204245'
2025-04-27 01:13:33,370 - Train: 9.00% [8200/91100]  [9.0/100.0] [batch_t 2.248 (2.248)] [data_t 0.251] [optim_t 1.997] [pixel 0.261 (0.261)]
2025-04-27 01:15:13,746 - Train: 9.06% [8250/91100]  [9.1/100.0] [batch_t 2.021 (2.007)] [data_t 0.004] [optim_t 2.018] [pixel 0.279 (0.255)]
2025-04-27 01:16:54,666 - Train: 9.11% [8300/91100]  [9.1/100.0] [batch_t 2.005 (2.018)] [data_t 0.004] [optim_t 2.002] [pixel 0.238 (0.248)]
2025-04-27 01:18:35,305 - Train: 9.17% [8350/91100]  [9.2/100.0] [batch_t 2.025 (2.013)] [data_t 0.004] [optim_t 2.021] [pixel 0.265 (0.250)]
2025-04-27 01:20:16,697 - Train: 9.22% [8400/91100]  [9.2/100.0] [batch_t 2.025 (2.028)] [data_t 0.004] [optim_t 2.022] [pixel 0.246 (0.250)]
2025-04-27 01:21:57,997 - Train: 9.28% [8450/91100]  [9.3/100.0] [batch_t 2.023 (2.026)] [data_t 0.004] [optim_t 2.020] [pixel 0.280 (0.248)]
2025-04-27 01:23:39,143 - Train: 9.33% [8500/91100]  [9.3/100.0] [batch_t 2.005 (2.023)] [data_t 0.004] [optim_t 2.001] [pixel 0.222 (0.252)]
2025-04-27 01:25:19,562 - Train: 9.39% [8550/91100]  [9.4/100.0] [batch_t 2.007 (2.008)] [data_t 0.003] [optim_t 2.003] [pixel 0.253 (0.248)]
2025-04-27 01:26:59,559 - Train: 9.44% [8600/91100]  [9.4/100.0] [batch_t 2.003 (2.000)] [data_t 0.003] [optim_t 1.999] [pixel 0.237 (0.243)]
2025-04-27 01:28:40,681 - Train: 9.50% [8650/91100]  [9.5/100.0] [batch_t 2.042 (2.022)] [data_t 0.004] [optim_t 2.039] [pixel 0.299 (0.246)]
2025-04-27 01:30:21,356 - Train: 9.55% [8700/91100]  [9.5/100.0] [batch_t 2.009 (2.013)] [data_t 0.004] [optim_t 2.006] [pixel 0.272 (0.243)]
2025-04-27 01:32:02,968 - Train: 9.60% [8750/91100]  [9.6/100.0] [batch_t 2.041 (2.032)] [data_t 0.004] [optim_t 2.038] [pixel 0.232 (0.249)]
2025-04-27 01:33:44,527 - Train: 9.66% [8800/91100]  [9.7/100.0] [batch_t 2.023 (2.031)] [data_t 0.004] [optim_t 2.020] [pixel 0.264 (0.250)]
2025-04-27 01:35:26,160 - Train: 9.71% [8850/91100]  [9.7/100.0] [batch_t 2.031 (2.033)] [data_t 0.004] [optim_t 2.028] [pixel 0.262 (0.243)]
2025-04-27 01:37:08,014 - Train: 9.77% [8900/91100]  [9.8/100.0] [batch_t 2.019 (2.037)] [data_t 0.004] [optim_t 2.016] [pixel 0.226 (0.240)]
2025-04-27 01:38:48,155 - Train: 9.82% [8950/91100]  [9.8/100.0] [batch_t 1.996 (2.003)] [data_t 0.004] [optim_t 1.992] [pixel 0.235 (0.245)]
2025-04-27 01:40:27,918 - Train: 9.88% [9000/91100]  [9.9/100.0] [batch_t 1.992 (1.995)] [data_t 0.003] [optim_t 1.988] [pixel 0.290 (0.250)]
2025-04-27 01:42:07,509 - Train: 9.93% [9050/91100]  [9.9/100.0] [batch_t 1.980 (1.992)] [data_t 0.004] [optim_t 1.976] [pixel 0.237 (0.240)]
2025-04-27 01:43:46,539 - Train: 9.99% [9100/91100]  [10.0/100.0] [batch_t 1.976 (1.981)] [data_t 0.003] [optim_t 1.973] [pixel 0.261 (0.239)]
2025-04-27 01:45:19,297 - Test: 1.75% [50/2865] [batch_t 2.310 (1.457)] [pixel 0.270 (0.262)]
2025-04-27 01:47:56,634 - Test: 3.49% [100/2865] [batch_t 3.968 (2.302)] [pixel 0.133 (0.269)]
2025-04-27 01:52:55,825 - Test: 5.24% [150/2865] [batch_t 7.119 (3.529)] [pixel 0.125 (0.225)]
2025-04-27 01:59:45,376 - Test: 6.98% [200/2865] [batch_t 9.268 (4.695)] [pixel 0.224 (0.205)]
2025-04-27 02:08:23,860 - Test: 8.73% [250/2865] [batch_t 11.416 (5.830)] [pixel 0.253 (0.209)]
2025-04-27 02:18:49,853 - Test: 10.47% [300/2865] [batch_t 13.562 (6.945)] [pixel 0.279 (0.212)]
2025-04-27 02:31:03,126 - Test: 12.22% [350/2865] [batch_t 15.706 (8.048)] [pixel 0.269 (0.219)]
2025-04-27 02:45:03,188 - Test: 13.96% [400/2865] [batch_t 17.869 (9.142)] [pixel 0.168 (0.218)]
2025-04-27 03:01:02,217 - Test: 15.71% [450/2865] [batch_t 20.358 (10.257)] [pixel 0.125 (0.209)]
2025-04-27 03:18:53,693 - Test: 17.45% [500/2865] [batch_t 22.445 (11.375)] [pixel 0.233 (0.207)]
2025-04-27 03:38:32,292 - Test: 19.20% [550/2865] [batch_t 24.624 (12.483)] [pixel 0.216 (0.213)]
2025-04-27 04:00:00,896 - Test: 20.94% [600/2865] [batch_t 26.867 (13.591)] [pixel 0.242 (0.216)]
2025-04-27 04:23:19,951 - Test: 22.69% [650/2865] [batch_t 29.014 (14.698)] [pixel 0.252 (0.219)]
2025-04-27 04:48:29,503 - Test: 24.43% [700/2865] [batch_t 31.248 (15.804)] [pixel 0.282 (0.223)]
2025-04-27 05:15:29,268 - Test: 26.18% [750/2865] [batch_t 33.417 (16.910)] [pixel 0.281 (0.227)]
2025-04-27 05:44:21,749 - Test: 27.92% [800/2865] [batch_t 35.908 (18.019)] [pixel 0.211 (0.229)]
2025-04-27 06:15:07,107 - Test: 29.67% [850/2865] [batch_t 37.932 (19.130)] [pixel 0.218 (0.228)]
2025-04-27 06:47:37,919 - Test: 31.41% [900/2865] [batch_t 39.930 (20.235)] [pixel 0.275 (0.229)]
2025-04-27 07:21:53,989 - Test: 33.16% [950/2865] [batch_t 42.081 (21.334)] [pixel 0.298 (0.231)]
2025-04-27 07:58:03,588 - Test: 34.90% [1000/2865] [batch_t 44.190 (22.437)] [pixel 0.185 (0.230)]
2025-04-27 08:35:54,936 - Test: 36.65% [1050/2865] [batch_t 46.471 (23.532)] [pixel 0.194 (0.229)]
2025-04-27 09:15:32,225 - Test: 38.39% [1100/2865] [batch_t 48.663 (24.623)] [pixel 0.285 (0.229)]
2025-04-27 09:57:06,371 - Test: 40.14% [1150/2865] [batch_t 51.153 (25.722)] [pixel 0.213 (0.230)]
2025-04-27 10:40:35,812 - Test: 41.88% [1200/2865] [batch_t 53.194 (26.824)] [pixel 0.211 (0.229)]
2025-04-27 11:26:03,637 - Test: 43.63% [1250/2865] [batch_t 55.067 (27.934)] [pixel 0.250 (0.228)]
2025-04-27 12:13:04,475 - Test: 45.38% [1300/2865] [batch_t 57.367 (29.029)] [pixel 0.258 (0.229)]
2025-04-27 13:02:04,895 - Test: 47.12% [1350/2865] [batch_t 60.125 (30.132)] [pixel 0.272 (0.230)]
2025-04-27 13:53:04,250 - Test: 48.87% [1400/2865] [batch_t 62.209 (31.241)] [pixel 0.267 (0.231)]
2025-04-27 14:45:54,428 - Test: 50.61% [1450/2865] [batch_t 64.321 (32.350)] [pixel 0.345 (0.232)]
2025-04-27 15:40:17,497 - Test: 52.36% [1500/2865] [batch_t 66.258 (33.447)] [pixel 0.320 (0.235)]
2025-04-27 16:36:32,022 - Test: 54.10% [1550/2865] [batch_t 68.703 (34.545)] [pixel 0.256 (0.237)]
2025-04-27 17:34:50,103 - Test: 55.85% [1600/2865] [batch_t 71.277 (35.652)] [pixel 0.240 (0.238)]
2025-04-27 18:35:11,726 - Test: 57.59% [1650/2865] [batch_t 73.544 (36.767)] [pixel 0.197 (0.238)]
2025-04-27 19:37:31,178 - Test: 59.34% [1700/2865] [batch_t 75.941 (37.885)] [pixel 0.240 (0.238)]
2025-04-27 20:41:42,870 - Test: 61.08% [1750/2865] [batch_t 78.153 (39.004)] [pixel 0.246 (0.237)]
2025-04-27 21:47:52,387 - Test: 62.83% [1800/2865] [batch_t 80.663 (40.125)] [pixel 0.255 (0.237)]
2025-04-27 22:56:00,729 - Test: 64.57% [1850/2865] [batch_t 82.226 (41.251)] [pixel 0.375 (0.238)]
2025-04-28 00:06:01,173 - Test: 66.32% [1900/2865] [batch_t 85.097 (42.376)] [pixel 0.328 (0.242)]
2025-04-28 01:17:54,588 - Test: 68.06% [1950/2865] [batch_t 86.905 (43.502)] [pixel 0.229 (0.243)]
2025-04-28 02:31:31,952 - Test: 69.81% [2000/2865] [batch_t 89.445 (44.623)] [pixel 0.252 (0.243)]
2025-04-28 03:46:55,747 - Test: 71.55% [2050/2865] [batch_t 91.501 (45.741)] [pixel 0.334 (0.245)]
2025-04-28 05:04:18,591 - Test: 73.30% [2100/2865] [batch_t 93.927 (46.863)] [pixel 0.307 (0.248)]
2025-04-28 06:24:12,678 - Test: 75.04% [2150/2865] [batch_t 96.176 (48.003)] [pixel 0.239 (0.248)]
2025-04-28 07:45:14,409 - Test: 76.79% [2200/2865] [batch_t 98.388 (49.122)] [pixel 0.235 (0.248)]
2025-04-28 09:08:09,940 - Test: 78.53% [2250/2865] [batch_t 100.128 (50.242)] [pixel 0.271 (0.248)]
2025-04-28 10:33:07,522 - Test: 80.28% [2300/2865] [batch_t 103.058 (51.366)] [pixel 0.250 (0.247)]
2025-04-28 12:00:40,337 - Test: 82.02% [2350/2865] [batch_t 106.036 (52.508)] [pixel 0.233 (0.248)]
2025-04-28 13:29:42,963 - Test: 83.77% [2400/2865] [batch_t 107.772 (53.640)] [pixel 0.239 (0.248)]
2025-04-28 15:00:11,348 - Test: 85.51% [2450/2865] [batch_t 108.948 (54.761)] [pixel 0.252 (0.248)]
2025-04-28 16:31:56,820 - Test: 87.26% [2500/2865] [batch_t 111.193 (55.868)] [pixel 0.337 (0.248)]
2025-04-28 18:05:41,143 - Test: 89.01% [2550/2865] [batch_t 114.347 (56.978)] [pixel 0.257 (0.248)]
2025-04-28 19:42:08,222 - Test: 90.75% [2600/2865] [batch_t 116.802 (58.108)] [pixel 0.284 (0.249)]
2025-04-28 21:20:16,600 - Test: 92.50% [2650/2865] [batch_t 118.816 (59.234)] [pixel 0.235 (0.249)]
2025-04-28 23:00:16,822 - Test: 94.24% [2700/2865] [batch_t 121.019 (60.359)] [pixel 0.339 (0.250)]
2025-04-29 00:42:10,530 - Test: 95.99% [2750/2865] [batch_t 123.321 (61.485)] [pixel 0.312 (0.252)]
2025-04-29 02:26:01,923 - Test: 97.73% [2800/2865] [batch_t 125.498 (62.613)] [pixel 0.236 (0.253)]
2025-04-29 04:11:37,701 - Test: 99.48% [2850/2865] [batch_t 128.011 (63.737)] [pixel 0.215 (0.253)]
2025-04-29 04:42:53,205 - Test: 100.00% [2865/2865] [batch_t 81.153 (64.058)] [pixel 0.257 (0.253)]
